<?xml version="1.0" encoding="utf-8"?>
<root>
  <!-- 
    Microsoft ResX Schema 
    
    Version 2.0
    
    The primary goals of this format is to allow a simple XML format 
    that is mostly human readable. The generation and parsing of the 
    various data types are done through the TypeConverter classes 
    associated with the data types.
    
    Example:
    
    ... ado.net/XML headers & schema ...
    <resheader name="resmimetype">text/microsoft-resx</resheader>
    <resheader name="version">2.0</resheader>
    <resheader name="reader">System.Resources.ResXResourceReader, System.Windows.Forms, ...</resheader>
    <resheader name="writer">System.Resources.ResXResourceWriter, System.Windows.Forms, ...</resheader>
    <data name="Name1"><value>this is my long string</value><comment>this is a comment</comment></data>
    <data name="Color1" type="System.Drawing.Color, System.Drawing">Blue</data>
    <data name="Bitmap1" mimetype="application/x-microsoft.net.object.binary.base64">
        <value>[base64 mime encoded serialized .NET Framework object]</value>
    </data>
    <data name="Icon1" type="System.Drawing.Icon, System.Drawing" mimetype="application/x-microsoft.net.object.bytearray.base64">
        <value>[base64 mime encoded string representing a byte array form of the .NET Framework object]</value>
        <comment>This is a comment</comment>
    </data>
                
    There are any number of "resheader" rows that contain simple 
    name/value pairs.
    
    Each data row contains a name, and value. The row also contains a 
    type or mimetype. Type corresponds to a .NET class that support 
    text/value conversion through the TypeConverter architecture. 
    Classes that don't support this are serialized and stored with the 
    mimetype set.
    
    The mimetype is used for serialized objects, and tells the 
    ResXResourceReader how to depersist the object. This is currently not 
    extensible. For a given mimetype the value must be set accordingly:
    
    Note - application/x-microsoft.net.object.binary.base64 is the format 
    that the ResXResourceWriter will generate, however the reader can 
    read any of the formats listed below.
    
    mimetype: application/x-microsoft.net.object.binary.base64
    value   : The object must be serialized with 
            : System.Runtime.Serialization.Formatters.Binary.BinaryFormatter
            : and then encoded with base64 encoding.
    
    mimetype: application/x-microsoft.net.object.soap.base64
    value   : The object must be serialized with 
            : System.Runtime.Serialization.Formatters.Soap.SoapFormatter
            : and then encoded with base64 encoding.

    mimetype: application/x-microsoft.net.object.bytearray.base64
    value   : The object must be serialized into a byte array 
            : using a System.ComponentModel.TypeConverter
            : and then encoded with base64 encoding.
    -->
  <xsd:schema id="root" xmlns="" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:msdata="urn:schemas-microsoft-com:xml-msdata">
    <xsd:import namespace="http://www.w3.org/XML/1998/namespace" />
    <xsd:element name="root" msdata:IsDataSet="true">
      <xsd:complexType>
        <xsd:choice maxOccurs="unbounded">
          <xsd:element name="metadata">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name="value" type="xsd:string" minOccurs="0" />
              </xsd:sequence>
              <xsd:attribute name="name" use="required" type="xsd:string" />
              <xsd:attribute name="type" type="xsd:string" />
              <xsd:attribute name="mimetype" type="xsd:string" />
              <xsd:attribute ref="xml:space" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name="assembly">
            <xsd:complexType>
              <xsd:attribute name="alias" type="xsd:string" />
              <xsd:attribute name="name" type="xsd:string" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name="data">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name="value" type="xsd:string" minOccurs="0" msdata:Ordinal="1" />
                <xsd:element name="comment" type="xsd:string" minOccurs="0" msdata:Ordinal="2" />
              </xsd:sequence>
              <xsd:attribute name="name" type="xsd:string" use="required" msdata:Ordinal="1" />
              <xsd:attribute name="type" type="xsd:string" msdata:Ordinal="3" />
              <xsd:attribute name="mimetype" type="xsd:string" msdata:Ordinal="4" />
              <xsd:attribute ref="xml:space" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name="resheader">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name="value" type="xsd:string" minOccurs="0" msdata:Ordinal="1" />
              </xsd:sequence>
              <xsd:attribute name="name" type="xsd:string" use="required" />
            </xsd:complexType>
          </xsd:element>
        </xsd:choice>
      </xsd:complexType>
    </xsd:element>
  </xsd:schema>
  <resheader name="resmimetype">
    <value>text/microsoft-resx</value>
  </resheader>
  <resheader name="version">
    <value>2.0</value>
  </resheader>
  <resheader name="reader">
    <value>System.Resources.ResXResourceReader, System.Windows.Forms, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089</value>
  </resheader>
  <resheader name="writer">
    <value>System.Resources.ResXResourceWriter, System.Windows.Forms, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089</value>
  </resheader>
  <data name="CatalogueTabListbox" xml:space="preserve">
    <value>This Tab lets you edit the descriptive metadata for your Catalogues (datasets).  Right clicking a Catalogue will bring up a menu with a list of Catalogue level operations e.g. choosing which of the CatalogueItems in the catalogue is most indicative of record time periodicity (e.g. admission date of a hostpital admissions record).

GETTING STARTED: If this is your first time launching the software it is likely that you do not yet have any Catalogues (datasets) available.  Your first step should be either:
- Import one of your datasets into the Catalogue Manager by using the 'TableInfo' tab -&gt; Import
OR
- run the 'Setup Test' via the 'Diagnostics-&gt;Launch Diagnostics Screen...' menu option (this will create a test dataset and example data load - including example dirty CSV file which can be loaded through the DataLoadEngine)</value>
  </data>
  <data name="HighlightButton" xml:space="preserve">
    <value>Similar to the 'TOP 1' button, this button tests whether Extraction SQL is available for each Catalogue.  This button will only perform a syntax check (ensuring that Extraction SQL can be generated) and will not actually execute the SQL (which is what the TOP 1 button does).

There are a number of reasons a dataset may fail Extraction SQL generation (in which case it will appear in red), these include:
	- Selecting from multiple underlying TableInfos without configuring JoinInfo
	- Misconfiguration/Bad Ordering of Lookup table columns
	- References to missing ColumnInfos (where Table structures have changed and been updated in the metadata resulting in an orphaned CatalogueItem).
	
Catalogues which do not have any Extraciton SQL configured appear in black.</value>
  </data>
  <data name="ImportTableInfo" xml:space="preserve">
    <value>This pane lets you choose a database table and automatically generated a TableInfo and ColumnInfos set of Technical metadata.  These objects can then be used to automatically create a new Catalogue or augment an existing Catalogue.  

A typical use case would be where you have two tables which are joined on a single primary/foreign key.  Each table would be selected in turn, the first would be used to generate a new Catalogue and the second would be added to the same Catalogue.  You would then configure a Lookup join and mark some of the CatalogueItems as extractable.

TableInfos and ColumnInfos exist in the Catalogue database and must by synchronized regularly (any time there is a change to the underlying database architecture).  To synchronize a TableInfo right click it and choose 'Synchronize TableInfo'.  Most applications that use TableInfos will automatically attempt to synchronize them.

It is important that the user account doing the importing has the rights to interrogate column types/collations etc</value>
  </data>
  <data name="OverwriteValues" xml:space="preserve">
    <value>There is a one to many relationship between Catalogues (Datasets) and CatalogueItems (Virtual Columns).  For example hospital admisisions has columns for 'patient identifier', 'date of admission', 'condition of patient etc.  This button will overwrite the Topic and Periodicity data of all CatalogueItems with the values in the Catalogue (the values in the controls to the left).  This can be used for example when all columns in a dataset are received at the same time e.g. monthly and no columns in the dataset have a different periodicity (e.g. a lookup calculated field might have a weekly periodicity if it's lookup table is refreshed more regularly than the dataset)</value>
  </data>
  <data name="TableInfoTabListbox" xml:space="preserve">
    <value>TableInfos contain the Data Catalogue's knowledge of tables under your control.  This includes the server,database, table name as well as some additional technical information e.g. which columns should be anonymised.

Each TableInfo has one or more ColumnInfos (one per Column in the underlying database).  These contain information some 'cached' information such as data type, , primary key status, collation etc.  ColumnInfos are also used as a source of configuring extractability at a Catalogue level.  A single ColumnInfo may be used in multiple different Catalogues (datasets) e.g. a lookup description of 'Gender' could be used by a Demography Catalogue, a Hospital Admissions Catalogue and many others.

It is important to regularly synchronize the TableInfo (any time there is a change to the underlying database architecture).  To synchronize a TableInfo right click it and choose 'Synchronize TableInfo'.  Most applications that use TableInfos will automatically attempt to synchronize them.

*The only time you should need to edit the metadata of a ColumnInfo/TableInfo manually is when there is a change of name of a field and you want to preserve the relationships to CatalogueItems (Normally Synchronizer will see a rename as a 'Delete ColumnInfo X' and 'Add New ColumnInfo Y' where X is the old name and Y is the new name (manually editing the metadata for rename operations will help avoid this).  Note that Synchronization will always ask you to confirm any changes it is about to make - so remain vigilant.</value>
  </data>
  <data name="TOP1Catalogues" xml:space="preserve">
    <value>The Top 1 button provides a way of testing whether the Extraction SQL configured for your datasets actually works against your live database.  It is possible to configure ExtractionInformation that is Syntactically correct (as far as the CatalogueManager is concerned) but which will fail when run e.g. reference a non existent scalar function.

Clicking this button will take each Catalogue (dataset) in turn and work out it's Extraction SQL.  This SQL is then given a TOP 1 constraint (to improve performance) and executed.  All datasets which receive an Exception (usually an SQL Syntax exception but possibly a Time-out exception if you have crazy slow views) appear in red.  

Once all Catalogues have been processed, a short report of the exceptions encountered are displayed.  Although you may choose to double click the red Catalogues individually so you can run the SQL yourself and debug it in an appropriate debugger.

Catalogues that appear in Black do not have any Extraction SQL configured</value>
  </data>
  <data name="WhatIsACatalogue" xml:space="preserve">
    <value>There is a one to many relationship between Catalogues (Datasets) and CatalogueItems (Virtual Columns).  For example, in the screenshot above, hospital admissions (isd_smr01) has columns for 'CHI' (patient identifier), 'ADMISSION_DATE', 'DATE_OF_MAIN_OPERATION' etc.  Select a Catalogue in order to view the CatalogueItems it contains.

Note that it is possible for there to be CatalogueItems that are named differently to their underlying database column, for example there could be a CatalogueItem Age which is calculated from a field called DateOfBirth</value>
  </data>
  <data name="WhatIsACatalogueItem" xml:space="preserve">
    <value>CatalogueItems are 'virtual columns' within a Catalogue (dataset).  They exist independently of underlying database architecture.  This allows you to define a CatalogueItem such as 'Admission Date' and populate it's descriptive data before you wire it up to an actual database column.

Later on if you rename or delete the underlying database column, the descriptive CatalogueItem is not affected and can be rewired to a new underlying column.  

In addition to providing a level of separation between descriptive column information and technical database architecture, CatalogueItems allow for the possibility of transforming a column.  There can for example be two CatalogueItems for a database column 'DateOfAdmission' (ColumnInfo), one could be called 'AgeAtAdmission' and the other 'DateOfAdmission' (verbatim).  The CatalogueItem 'AgeAtAdmission' would have a transform which calculated age based on Patient Identifier and the value in 'DateOfAdmission'.</value>
  </data>
  <data name="WhatIsAColumnInfo" xml:space="preserve">
    <value>ColumnInfos are database columns.  They map to actual database columns in a database table.  All ColumnInfos belong to a parent TableInfo that contains information about the underlying database Table.  ColumnInfos/TableInfos contain information that can be automatically calculated from the database e.g. DataType, Format (e.g. Collation), Primary Key status etc.  

In addition to this ColumnInfos/TableInfos contain information such as:
	- How a table is joined during lookup operations (Left/Right/Outer/Lookup etc)
	- Which columns are anonymised
	- Which tables should be used as the root table for a multi table join (i.e. where a Catalogue spans multiple underlying tables which must be joined)
	
The CatalogueItems Tab allows you to configure which ColumnInfos relate to which CatalogueItems and perform bulk operations such as deleting all MISSING associations (i.e. where a ColumnInfo has been deleted resulting in an orphaned CatalogueItem).</value>
  </data>
  <data name="IsPrimaryExtractionTable" xml:space="preserve">
    <value>Sometimes (rarely) it is necessary to designate a TableInfo as a Primary Extraction Table.  In the above example there are 3 tables, the Person table is joined to the Address and Car tables via relationships.  Assume these are imported into the Data Catalogue under a single Catalogue (Person) and configured as Extractable columns (See CatalogueItem Tab).  QueryBuilder is unable to generate Extraction SQL.

The reason that Extraction SQL cannot be generated initially is because of a variance in results i.e. if the JOIN chain starts at Address then we will loose all records for homeless people.  If the JOIN chain starts at Car then we loose all people who do not own cars.  In the above example we choose for Person to be the 'Primary Extraction Table', resulting in a dataset of all People regardless of possessions (Cars/Houses) - We also omit from the result set any Cars/Addresses that are not owned by anyone (but that is intended).

After enabling the checkbox for Person, we can see that the QueryBuilder (accessible by double clicking the Catalogue in either CatalogueItem Tab or Catalogue Tab) can produce the desired SQL and that the join chain starts at Person.</value>
  </data>
  <data name="InternalOnlyRadio" xml:space="preserve">
    <value>Live/Deprecated and Internal/Public are two flag states a Catalogue (dataset) can be in.  By default the radio buttons are set such that only 'public','live' Catalogues are visible.  If you want to see 'public' deprecated Catalogues, switch the Deprecated radio button.

It is preferred that you never delete a Catalogue when you have used it for DataLoads or DataExtractions because other entities/programs within the framework may have references to the Catalogue ID and will not be happy if Catalogues start vanishing (although they will usually be quite graceful about telling you this)).

The Catalogue Right-Click menu lets you change the Live/Deprecated and Internal/Public flags - this will also result in a refresh of the Catalogue Manager such that the dataset will no longer appear until you change the radio buttons to reflect it's new state.</value>
  </data>
  <data name="ChoosePlatformDatabase" xml:space="preserve">
    <value>The Catalogue database holds all the information about datasets that you can see in the CatalogueManager, this includes 

Descriptive data,Load logic,Technical knowledge about underlying tables (joins, collations etc),Filter logic,Aggregate generation logic,Validation logic,Issues,Supporting Documents,Extraction transforms etc

If you are seeing this screen then you either opted to change platform database settings or the program detected a problem with the currently configured settings and is giving you an opportunity to change them (they might be blank even!)

You MUST either provide an SQL Server connection string to an existing Catalogue database, OR create a new empty Catalogue (click 'Create New...').

The Data Export Manager database stores projects, cohorts and configurations for extracting data from your database (see DataExportManager application).  This data is stored in a different database, population of this connection string is optional and if it is left blank then most of the DataManagementPlatform will still work but you won't be able to extract data through the DataExportManager.

These connection strings are stored in the Windows Registry of each system in:
HKEY_CURRENT_USER\Software\HICDataManagementPlatform</value>
  </data>
  <data name="ManageExternalServers" xml:space="preserve">
    <value></value>
  </data>
  <data name="DiagnosticsScreen" xml:space="preserve">
    <value>The diagnostics screen lets you check for catastrophic problems with your Catalogue or DataExportManager databases (missing fields, missing tables etc).  Selecting one of the 'checks' buttons will bring up a series of results indicating successful checks and failed checks.  Any checks that appear as Ex have Exceptions associated with them, you can double click an Ex entry to view more detail including a StackTrace (which can be handy for bug reporting)

In addition this screen lets you create (or reset/recreate) a set of test entries including:
A Test database and test table
A Catalogue entry for the test table
A load configuration with an example dirty CSV flat file

Once you have run this test setup you can test the DatasetLoader to load the example CSV file into the test database.  Once you have seen the records loaded you can further add a DataExportManager test extraction configuration (and Cohort)</value>
  </data>
  <data name="AggregateForceJoins" xml:space="preserve">
    <value>By default the QueryBuilder will automatically determine which tables need to be joined in order to execute a given aggregate.  This is done by finding JoinInfo for all the tables that are in the 'selected dimensions' list.

But sometimes you want to Join on a table even though it is not included in the 'Selected Dimensions' list.  For example (see diagram), you want an aggregate of all the events in a given country but if you only aggregate countries you will get a distinct list of countries only (with a count of 1 under each country).  By forcing a join to the events table we can make the Aggregate execute across both tables and give us the proper count of events in each country.</value>
  </data>
  <data name="ANOTablesConfiguration" xml:space="preserve">
    <value>An ANOTable is an identifier mapping table that stores identifiable values and anonymous substitutions.  This allows you to split identifiers (such as CHI, GP code, Location Codes etc) into their own data repository (database) leaving only the actual research data (condition codes, admission dates etc) in main repository.  ANOTabling is different from identifier dumping (in which fields such as Forename,Surname etc are dropped entirely) because a substitution is performed (to the anonymous version) allowing you to still do linkages (for example by Sending Location).

The first step in having an ANO substitution table is to create an ANO Server, this is done from the main Catalogue Form (Locations-&gt;Manage Known External Servers).  Once you have configured your ANO server the steps are as follows:
1a. Because ANO Servers are external isolated self-contained systems you must connect to it and let the Catalogue Manager discover available Tables
1b. OR you can start by right clicking a column and creating a new ANOTable directly from the column 
2.  If you think you already have a similar format ANOTable you can choose to add an existing ANOTable substitution and use that.  This is often the case where you have a specific type of data e.g. GP_Code which appears in multiple different datasets (and/or different columns within the same dataset) and you want to use the same anonymisation on all columns of the type to allow cross linkage of the anonymous form of the identifier.
3. Once you have an ANOTable in your work area (see 1b or 2) then you can assign columns to it by dragging them from the left and dropping them on the ANOTable box in the diagram</value>
  </data>
  <data name="HotVsColdStorage" xml:space="preserve">
    <value>Hot/Cold is a Catalogue (Dataset) level flag that is designed to allow you to easily distinguish between your main datasets which you frequently extract and are well maintained and those that are maintained for niche extracts and may be poorly or completely undocumented.  

Examples of candidates for Cold Storage include researcher datasets e.g. Researcher X wants you to store a 5,000 questionnaire responses in a table (5,000 rows 200 columns), he doesn't give you descriptions for the columns but wants you to release subsets of dataset at a later date once he has identified suitable cohorts.

		-"You know that questionnaire I gave you 4 years ago, can you give me 'Hospital Admissions', 'Demography' and the questionnaire responses for these 10 people who I discovered in a case-note review, I'm pretty sure they all had questionnaires but some of them might not have"
		
By marking such a dataset 'cold storage', it doesn't clutter up your view of your main Catalogues but it is still available for linkage/extraction later when the research comes back 4 years later wanting a copy of his data.</value>
  </data>
  <data name="ConfigureLookups" xml:space="preserve">
    <value>This interface lets you quickly define multiple lookup tables and incorporate them into the extraction of a dataset.  If you only want a specific lookup table (perhaps with many columns) to be extracted alongside your dataset then you should use 'Supporting SQL' instead (Right Click a Catalogue).  

If however you have some codes that you want to allow inline description then this interface is for you! (e.g. you want the ability to extract "Bob,105,2001-01-01,T,Tayside"  where 'T' is a foreign key into a lookup table).

Begin by entering the server/database containing your lookup table (this must be on the same server as your dataset but does not need to necessarily be in the same database).  Then drag the lookup table primary key up to the text box above it (A primary key is usually the Code part of the lookup e.g. 'T').  Then drag the Description part of the lookup onto the foreign key in the main dataset (e.g. drag the field with 'Description' and drop it onto the foreign key field 'PatientsGeographicalArea').

Note that you can use the same lookup table multiple times within the same dataset e.g. if you have a location code for where an incident occurred and another location code for where the patient was discharged to.  You do not need to do anything special when reusing lookups throughout your column set, instead the system will work out what keys need joined and appropriate lookup table aliases at SQL generation time by looking at the column orders (meaning you CAN have Code1,Code1_Desc,Code2,Code2_Desc but you CANT have Code1,Code2,Code2_Desc,Code1_Desc).

IMPORTANT: once you have configured your lookups always go back to the main screen and double click your Catalogue to view and test the new extraction SQL.</value>
  </data>
  <data name="ConfigureCredentialsAddEdit" xml:space="preserve">
    <value>Credentials allow certain users to access different areas of the platform and are designed to be set up once and then be used in the background seamlessly during checks. Credentials cannot be deleted if they have a relationship with a table info; the link must be broken between them before a set can be removed.</value>
  </data>
  <data name="ConfigureCredentialsBreakLinkage" xml:space="preserve">
    <value>Linkages between credentials and tableinfos can be broken by pressing the delete key while highlighting the linked tableinfo/credentials. </value>
  </data>
  <data name="ConfigureCredentialsExistingRelationships" xml:space="preserve">
    <value>Once you have selected a set of credentials and a tableinfo, you can choose what context to tie them together with. Users must use credentials with the correct context to perform certain actions and will be denied continuation if they are wrong.</value>
  </data>
  <data name="ConfigureCredentialsPermissionsVisualiser" xml:space="preserve">
    <value>The permissions visualiser lets you see the various relationships a set of credentials and a table info might have.</value>
  </data>
  <data name="SupportingSqlTableViewer" xml:space="preserve">
    <value>The Data Catalogue can store and extract unstructured data tables.  These might be lookup tables or audit logs or any other data you want to bundle along with your extracts.

It is important to remember that this is UNSTRUCTURED and should NOT be used for proper datasets (Catalogues).  It is especially important that you do not write queries that expose private identifiers as SupportingSQLTables are extracted verbatim and do not undergo project anonymisation.

After entering the SQL, a description and name you must also configure the ExternalDatabaseServer on which the SQL should be run.</value>
  </data>
  <data name="SupportingDocumentsViewer" xml:space="preserve">
    <value>Supporting documents are useful binary documents such as PDF files, Word Documents etc.  These documents support understanding particular datasets and might often be provided by a third party e.g. the metadata provided to you by a dataset supplier.  You should extract knowledge from these files and integrate them into the Catalogue and CatalogueItem descriptions but having the original attachement available too is also useful.

If you check Extractable then it will make the document available to researchers when you perform data extractions in Data Export Manager.</value>
  </data>
  <data name="TicketingControl" xml:space="preserve">
    <value>This control lets you reference a ticket in your ticketing system (e.g. JIRA, fogbugz etc).  The control has a location for you to record the ticket identifier (e.g. LINK-123) and a button for launching the ticketing system (exact implementation of this will depend on how you have configured the TicketingSystemConfiguration).

This control is used for storing a ticket against an object in the database e.g. a researchers dataset extraction request or a bug report about a column in a dataset.</value>
  </data>
  <data name="IssueUI" xml:space="preserve">
    <value>Tracking problems you have with your datasets is important and the Data Catalogue allows for this through the Issue system.  Issues are tied to CatalogueItem (not ColumnInfo) objects, this allows there to be issues with a given transform e.g. a problem with 'CleanAdmissionCode' but no problem with 'AdmissionCode' even though both CatalogueItems own the same underlying ColumnInfo.

You can tie your issues in the Catalogue to your existing ticketing/work logging system (e.g. JIRA or Fogbugz) by setting up a TicketingSystemConfiguration and populating the Tracking value (in the top right).

IMPORTANT: By default these issues are reported in the Metadata that is provided to researchers when the datasets (Catalogues) are extracted so be careful to be clean and consise in your descriptions and notes to researchers.

To generate a report of all current issues in the Catalogue, go to the Main Form and select Reports=&gt;Generate Report=&gt;Issue Report</value>
  </data>
  <data name="SetupTestDataset" xml:space="preserve">
    <value>The Diagnostic screen allows you to setup a 'Test Dataset'.  This creates an example dataset in an SQL Server Database, imports it into the Catalogue database as a new dataset and creates some additional artifacts such as a data load configuration, an extraction configuration (optionally anonymisation too) and some attachments / extraction logic / filters etc.

This gives you a sandbox in which to explore the capabilities of the RMDP software and learn it's strengths and weaknesses.  After you have played around with the test dataset and made some configuration alterations you might decided you want to delete the sandbox and start again.  There are two ways of doing this, the nuclear option is to delete all of your databases (ANO, Logging, Catalogue, Data Export, DMP_Test etc) and have Catalogue Manager reinstall everything over BUT if you don't fancy that option you can simply run 'Create Test Dataset' again, this will try to clean up any remnants of the previous run and re-create the test dataset.  However because of the relational structure of RDMP it may not be possible, for example if you have used the ANO/Logging database for a real dataset then deleting and recreating it would be a terrible idea so the system will not perform that step.</value>
  </data>
  <data name="CatalogueDataModel" xml:space="preserve">
    <value>This diagram explains the 5 core data objects that drive CatalogueManager and DataExportManager.

A Catalogue is a product that can be made available to researchers, it has descriptive metadata, attachments etc.
A CatalougeItem is a column that the researcher will see in his extracted product, it may be directly extracted or it may be transformed in some way (e.g. data cleaning on extract).
An ExtractionInformation is a declaration that a given CatalogueItem is powered by a given ColumnInfo and it describes in plain SQL how the extraction takes place (whether it is verbatim or a transform).
A ColumnInfo is a reference to a Column that exists in your SQL Server.
A TableInfo is a reference to a Table that exists in your SQL Server.

In the diagram above:
Case 1 - The usual case, there is 1 column in your SQL Server database which is extracted (ExtractionInformation) and made available to researchers (CatalogueItem).
Case 2 - Shows how there can be 2 versions of the same underlying ColumnInfo.  There is 1 ColumnInfo (1 source column in your SQL Server) but there are 2 ExtractionInformations (one could be verbatim while the other pushes the data through a cleaning algorithm).  Each ExtractionInformation has 1 corresponding CatalogueItem which describes the product/transform to the researcher who will ultimately receive the product.
Case 3 - This ColumnInfo has no ExtractionInformation/CatalogueItem meaning that it is not available for extract.  It may be an internal audit field or simply something that is too sensitive to release / irrelevant to researchers.

Any given TableInfo is likely to contain examples of all 3 of these cases side by side amongst it's ColumnInfos.</value>
  </data>
  <data name="ExerciseData" xml:space="preserve">
    <value>This dialog lets you generate interesting test data in which to practice tasks such as importing data, generating cohorts and performing project extractions.  Note that ALL the data generated is completely fictional.  Test data is generated randomly usually around a distribution (e.g. there are more prescsriptions for Paracetamol/Aspirin than Morphine) but complex relationships are not modelled (e.g. theres no concept of someone being diabetic so just because someone is on INSULIN doesn't mean they will have diabetic blood tests in biochemistry).  Likewise don't be surprised if people change address after they have died.

Identifiers are created from a central random pool (see step 1) and will be unique.  This means if you generate test data and then generate more tomorrow you are likely to only have very minimal intersection of patient identifiers.  For this reason it is important not to generate and load Prescribing one day and then generate and load Biochemistry the next day (instead you should generate all the data at once and use that as a reusable asset).</value>
  </data>
  <data name="Governance" xml:space="preserve">
    <value>The RDMP was designed and implemented to hold sensitive health datasets generated by third party data controllers and in anonymous form for reasearch purposes.  To this end it has a fully featured Governance system for tracking third party permission to hold datasets.  This usually takes the form of a collection of documents (e.g. PDFs and Emails) between your agency and your data providers which grants you permission to hold the datasets for specific periods.  It may be that you don’t need permission to hold particular datasets or that some permissions are granted once and last forever (never expire) – RDMP supports both of these scenarios too.

Consider the following scenario.  Your agency holds 2 datasets Biochemistry and Prescribing.  These datasets are sent to your agency by two healthboards (Tayside and Fife) both of which require annual approval letters.  Each year the RDMP alerts you to the fact that Governance has expired on your datasets, your governance manager sends an email to the relevant healthboards and receives new approval letters.  He then updates the system to have a new governance period for the new year and attaches the relevant documents to this new year.

Now consider that a new dataset becomes available (Virology in 2003).  Rather than just sneaking this dataset into the approval request list, a separate email chain is sent to both Tayside and Fife for approval to add the dataset and to integrate it into the main approvals list.  In this case there is a special one off governance period for 2003 which covers only Virology and reflects the permission to add this dataset to the repositry of data you hold.  In subsequent years Virology is covered by the normal annual approvals.</value>
  </data>
  <data name="ImportFlatFile" xml:space="preserve">
    <value>This dialog will let you import a flat file and create a Catalogue, this is done through the creation of a new / reuse of an existing Pipeline.  If this is your first time then most likely a simple pipeline with a CSV / Excel source will be all you need.  But the functionality exists to write your own 'freaky file type' plugin sources or perform 'load time substitutions' (e.g. Indexer/Linker) or do just about anything else your agency requires you to do.</value>
  </data>
</root>